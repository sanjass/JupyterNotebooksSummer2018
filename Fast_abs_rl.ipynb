{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Text Summarization\n",
    "#### Sanja Simonovikj - Shell internship, summer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have used extractive text summarization techniques and algorithms, such as TextRank (graph-based algorithm) or ElasticSearch (query-based summarization). These approaches gave relatively good summaries in the form of extracted relevant and important sentences of the document/s. However, there are limitations such as incomplete coverage or redundancy and sometimes lack of coherence between sentences. \n",
    "\n",
    "In this document we will provide overview of an abstractive method for text summarization, presented in a very recent paper\n",
    "\n",
    "**\"Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\"**. \n",
    "\n",
    "- The paper can be found here: https://arxiv.org/abs/1805.11080\n",
    "- Abstract:\n",
    "> Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.\n",
    "- The github code is available here: https://github.com/ChenRocks/fast_abs_rl\n",
    "> a pretrained model is available to be used to generate summaries on newly provided documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main idea:**\n",
    "- *The Extractor* (called f) extracts salient sentences. \n",
    "- *The Abstractor* (called g) rewrites each sentence and generates new words from a large vocabulary.\n",
    "- The extractor and abstractor are first trained separately, by optimizing ML (maximum-likelihood) objective and then RL is used to train the full model end-to-end (to aproximate the so-called model h) by applying policy gradient techniques.\n",
    "- The following is true: In any given pair of document and summary, every summary sentence can be produced from some document sentence. This underlying assumption is very important as it is the base of how our training data is constructed.\n",
    "\n",
    "\n",
    "**Extractor details:**\n",
    "- The extractor uses embedded word vectors and temporal convolutional model to get representation for each individual sentence in the documents. Then bidirectonal LSTM-RNN is applied to incorporate global context i.e. to get sentence representation which takes into account all the previous and future sentences. The extractor then selects sentences based on the obtained representations. LSTM-RNN is used to train a Pointer Network.\n",
    "- *Extractor Training:* The summarization dataset used does not have saliency labels for each sentence. In order to get labels indicating whether a certain sentence should be extracted or not, the following approach is used: for each ground-truth summary sentence we find the most similar document sentence using ROUGE-L (longest common subsequence) metric. This gives us labels so that we can have labeled training data to train the extractor.\n",
    "\n",
    "**Abstractor details:**\n",
    "- The abstractor approximates g, which compresses and paraphrases an extracted document sentence to a concise summary sentence. A standard encoder-aligner-decoder network is trained to minimize the cross-entropy loss of the decoder language model at each generation step.  At a high-level, it is a sequence-to-sequence model with attention  and  copy  mechanism  (but  no  coverage).\n",
    "- *Abstractor Training:* The training pairs for the abstractor are created by taking each ground-truth summary sentence and pairing it with its extracted document sentence (same as in training the extractor). \n",
    "\n",
    "**End-to-end training details:**\n",
    "- Reinforcement Learning (RL) is used to train the full model. The extractor is considered the RL agent. Note that the agent is sentence-level as opposed to word-level. The reward is a ROUGE-L score between the final generated sentence (after going through extractor + abstractor) and the associated ground-truth summary sentence. While doing RL training, we keep the abstractor parameters fixed. This implies that the extraction is reinforce-guided. \n",
    "- To limit the length of the produced summary a \"stop\" action is added in the policy action space. After reaching this stop action, the agent receives 0 reward. This means that the length of the produced summary is not customazable when using a pretrained model. The length is one of the parameters being learned. If we want to change this we would need to retrain the whole model, with training data containing reference (ground-truth) summaries that have the desired length. With the provided pre-trained model, we get ~4 sentences as a summary, no matter the length of the input document.\n",
    "\n",
    "**Repetition-avoiding reranking:**\n",
    "- An optional strategy to mitigate the issue of redundancy is by using reranking. However, the extractor already takes care of the redundancy issue by selecting non-redundant sentences. The reranking mechanism can slightly improve the outcome by removing a few \"across-sentence\" repetitions.\n",
    "\n",
    "\n",
    "### Training speed:\n",
    "As reported in the paper: \n",
    "> \"It took a total of 19.71 hours to train our model. 4.15 hours for the abstractor, 15.56 hours for the  RL training. Extractor ML training can be run at the same time with abstractor training and is approximately 1.5 hours. \"\n",
    "\n",
    "The code is written in python3 using PyTorch library with GPU and CUDA enabled installation. However, to run a pretrained model we can simply use CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Below we are presenting the generated summaries on our own data using a pretrained model. The model was trained as in the paper, using reranking strategy, on the CNN/DailyMail news dataset. To generate these summaries, the beam size was set to 5, batch to 1 (this solves the interleaving bug) and the other parameters were set to default.\n",
    "\n",
    "#### example 1:\n",
    "Raw text:\n",
    "> Russia remained China's largest crude supplier in February, with exports rising by 199,000 b/d to 1.32 million b/d, thanks to the recent start-up of a new crude pipeline between the two countries.\n",
    "Angola's exports to China also rose sharply, up 14.7% to 978,000 b/d, squeezing Saudi Arabia's supplies, which fell 2.9%, according to Chinese customs data (IOD Feb.27'18).\n",
    "Although US exports in February did not reach the previous month's record high, they still stood at a healthy 238,000 b/d.\n",
    "Going forward, the extent of crude trade between the two countries will be watched keenly amid heightened trade tensions.\n",
    "Venezuela's exports to China dropped sharply year-on-year, and compared to January, in a likely sign of growing upstream struggles in the Latin American country (EC Mar.23'18).\n",
    "China's February crude import data alone does not give an entirely accurate picture, with the New Year holiday having shut down most of the country for the second half of the month.\n",
    "While February crude imports edged up 1.5% from the previous year, combined imports for January and February surged by 10.8%, or 885,000 b/d, year-on-year to 9.06 million b/d (IOD Mar.12'18).\n",
    "\n",
    "Produced summary:\n",
    "> russia remained china 's largest crude supplier in february .\n",
    "angola 's exports to china rose sharply , up 14.7 % to 978,000 b/d .\n",
    "us exports in february did not reach record high .\n",
    "crude trade between the two countries will be watched keenly amid heightened trade tensions .\n",
    "\n",
    "#### example 2:\n",
    "Raw text:\n",
    "> Total has secured nonoperated interests in two exploration blocks offshore Guyana after acquiring the right to farm into the Orinduik Block last year.\n",
    "The French major has agreed to acquire a 35% stake in the deepwater Canje Block from Canada's JHI Associates and local firm Mid-Atlantic Oil & Gas, which will retain a 30% interest.\n",
    "The tract is operated by Exxon Mobil (35%) and lies in water depths ranging from 1,700 to 3,000 meters.\n",
    "Total has also bagged a 25% interest in the shallow-water Kanuku Block in water depths of 70 to 100 meters, under an agreement with operator Repsol (37.5%) and Tullow Oil (37.5%).\n",
    "The company previously acquired an option to buy a 25% stake in the nearby shallow-water Orinduik tract -- operated by UK-based Tullow (60%) -- from Canadian independent Eco Atlantic, which will keep a 15% stake.\n",
    "Guyana is set to become a significant oil producer once Exxon starts production in 2020 from the prolific Stabroek Block, where it has made a string of discoveries in recent years.\n",
    "Exxon has identified as many as 3.2 billion barrels recoverable from Liza and other discoveries, including Liza Deep, Payara, Turbot and Snoek (EIF Jan.10'18).\n",
    "\n",
    "Produced summary:\n",
    "> the french major has agreed to acquire a 35 % stake .\n",
    "the tract is operated by exxon mobil and lies in water depths .\n",
    "total has also bagged a 25 % interest in the shallow-water kanuku block .\n",
    "the company previously acquired an option to buy a 25 % stake in the nearby shallow-water orinduik tract .\n",
    "\n",
    "\n",
    "#### example 3:\n",
    "Raw text:\n",
    "> Britain's largest labour union, Unite the Union, announced series of 24 hour and 12 hour rig worker strikes at Total's North Sea oil and gas platforms, a statement said on Thursday.\n",
    "Details to follow:\n",
    "The three platforms will be forced to cease production, the statement said\n",
    "There will be three 24 hour stoppages on July 23 and Aug. 6 and 20 as well as two 12 hour stoppages on July 30 and Aug. 13\n",
    "The union also announced a ban on overtime starting on July 23\n",
    "The strike will affect Total's Alwyn, Dunbar and Elgin-Franklin platforms\n",
    "Union workers voted in favour of strike action on June 28 over work shifts and pay\n",
    "Reporting By Julia Payne \n",
    "\n",
    "Produced summary:\n",
    "> britain 's largest labour union , unite the union , announced series of 24 hour strikes .\n",
    "the three platforms will be forced to cease production .\n",
    "there will be three 24 hour stoppages on july 23 and aug. 6 .\n",
    "union also announced a ban on overtime starting on july 23 .\n",
    "strike will affect total 's alwyn , dunbar and elgin-franklin platforms .\n",
    "details to follow : \n",
    "\n",
    "\n",
    "#### example 4:\n",
    "Raw text:\n",
    "> Shipments of CPC Blend crude oil are set to scale new heights in March, with some 1.3 million b/d due to load at the Yuzhnaya Ozerreyevka terminal near the Russian Black Sea port of Novorossiysk, according to the latest schedule (NC Feb.15'18).\n",
    "The main reason for the uptick is higher volumes going into the Caspian Pipeline Consortium (CPC) pipeline from Kazakhstan's giant Kashagan field, which is set to contribute some 260,000 b/d this month, almost 100,000 b/d more than the amount scheduled for February.\n",
    "The marketing of the Kashagan barrels is assigned to the seven shareholders in the North Caspian Operating Co. (NCOC), although some of the rights are handed over to third parties.\n",
    "For example, Total handles Exxon Mobil's share of production, while Swiss trader Vitol lifts the barrels belonging to KMG Kashagan B.V. under a four-year offtake deal.\n",
    "Most of the CPC Blend ends up in Europe, with Italy the most popular destination.\n",
    "In recent months, CPC has found a home in the Baltics, with Royal Dutch Shell putting at least one cargo into Gdansk, Poland, and Chevron delivering into the Lithuanian port of Butinge.\n",
    "\n",
    "Produced summary:\n",
    "> shipments of cpc blend crude oil are set to scale new heights in march .\n",
    "the uptick is higher volumes going into the pipeline from kazakhstan 's giant kashagan field .\n",
    "the marketing of the kashagan barrels is assigned to the seven shareholders in the north caspian operating co. -lrb- ncoc -rrb- .\n",
    "in recent months , cpc has found a home with royal dutch shell .\n",
    "\n",
    "\n",
    "#### example 5:\n",
    "Raw text:\n",
    "> INDONESIA -- Chevron has submitted a revised development plan for the second stage of its Indonesia Deepwater Development (IDD), with costs said to be reduced by 25% from the original \\$12 billion.\n",
    "The US major also submitted a proposal to extend the contracted areas related to the IDD scheme.\n",
    "The firm has had to rework and resubmit its development plan several times, delaying the start-up of the phase from 2020 to 2022-23 (PIW Jan.18'16).\n",
    "One previous plan was rejected by the Indonesian government for requiring too many fiscal incentives, but Chevron has been able to significantly work down the project's capital and operational costs for the Gehem, Gendalo, Gandang and Maha fields under its latest development plan proposal.\n",
    "The first stage of IDD came on stream in 2016.\n",
    "At its peak, IDD is expected to produce 1.1 Bcf/d (11.4 Bcm/yr), with most of the gas earmarked for delivery to Indonesia's Bontang LNG plant in East Kalimantan.\n",
    "\n",
    "Produced summary:\n",
    "> indonesia has submitted a revised development plan for the second stage of its indonesia deepwater development .\n",
    "us major also submitted proposal to extend contracted areas related to idd scheme .\n",
    "the firm has had to rework and resubmit its development plan several times .\n",
    "chevron has been rejected by the indonesian government for requiring too many fiscal incentives ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pre-trained model\n",
    "To produce summaries for our documents, download the code and follow the instructions on https://github.com/ChenRocks/fast_abs_rl under \"Decode summaries from the pretrained model\". Note that the raw documents need to be pre-processed before using the model. The instructions for preprocessing can be found here: https://github.com/ChenRocks/cnn-dailymail. The code in \"make_datafiles.py\" is customzed to work on CNN/DailyMail dataset. We should change it to suit our own needs. The local version is saved in \"newest_make_datafiles.py\". The main file that produces the final summaries is \"decode_full_model.py\". Make sure you run \"export DATA=path/to/decompressed/data\" before running the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible bugs/incompatibilities:\n",
    "Here are some potential errors you can run into and how to solve them:\n",
    "- **\"RuntimeError: get_device is not implemented for type torch.FloatTensor\"**\n",
    "> replace every instance of something.**get_device()** with **\"cpu\"**. Example:\n",
    "\n",
    "```python \n",
    "#token, states = bs.pack_beam(beam, article.get_device())\n",
    "token, states = bs.pack_beam(beam, \"cpu\")\n",
    "\n",
    "#ind = torch.LongTensor(ind).to(attention.get_device())\n",
    "ind = torch.LongTensor(ind).to(\"cpu\")\n",
    "```\n",
    "- **\"AttributeError: module 'torch._C' has no attribute '_cuda_getDevice'\"**\n",
    "> in \"decoding.py\" in class \"DecodeDataset\" in \"load_best_ckpt\" method, change\n",
    "\n",
    "```python\n",
    " ckpt = torch.load(join(model_dir, 'ckpt/{}'.format(ckpts[0])))['state_dict']\n",
    "```\n",
    "to\n",
    "```python\n",
    "ckpt = torch.load(join(model_dir, 'ckpt/{}'.format(ckpts[0])), map_location='cpu')['state_dict']\n",
    "```\n",
    "\n",
    "- **\"ValueError: length of all samples has to be greater than 0, but found an element in 'lengths' that is <=0\"**\n",
    "> In \"decoding.py\" in class \"Abstractor\" in \"_prepro\" method change:\n",
    "\n",
    "```python\n",
    "articles = conver2id(UNK, self._word2id, raw_article_sents)\n",
    "#add the following line\n",
    "articles = [art for art in articles if len(art)!=0 ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
