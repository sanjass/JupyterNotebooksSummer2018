{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shell internship - summer 2018 - Sanja Simonovikj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This document is intended to summarize the results obtained by trying different algorithms and their variations for Text Summarization. Out initial approach is trying unsupervised learning by using extractive methods to generate generic, indicative, single-document summaries.\n",
    "\n",
    "Most of the current research is focused on extractive, as opposed to abstractive summarization. Extractive summarization is much easier to implement than abstractive summarization, as the latter requires extensive natural language processing. Purely extractive summaries often give better results compared to automatic abstractive summaries (ref: https://www.aaai.org/Papers/JAIR/Vol22/JAIR-2214.pdf ). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different approaches and evaluate the algorithms with various metrics and compare the results.\n",
    "For every approach, we will have description and tables to summarize the parameters, as well as to expose the obtained numerical results based on the evaluation metric used. One can find code, images, references as supplemental material if one wants to replicate or futher analyze the approach.\n",
    "\n",
    "Note: To see the code to generate the tables in this document, click the toggle switch named \"Click here to toggle on/off the raw code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** is the de facto standard in summarization evaluations. ROUGE has been chosen as the official automatic evaluation package for Document Understanding Conference since 2004. It is a package containing different metrics to evaluate system generated summaries given reference summaries.\n",
    "\n",
    "Some of the metrics are the following:\n",
    "- ROUGE-L: Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
    "- ROUGE-SU: Skip-bigram plus unigram-based co-occurrence statistics\n",
    "- ROUGE-1 refers to the overlap of 1-gram (each word) between the system and reference summaries\n",
    "- ROUGE-2 refers to the overlap of bigrams between the system and reference summaries\n",
    "\n",
    "In our evaluations we have used **ROUGE-2-1.2.1** which is an improved version of the initial one. The package can be downloaded from the following link, which also contains full documentation on usage: http://rxnlp.com/rouge-2-0/#.Wx-WS3WuzCI\n",
    "\n",
    "Read more about ROUGE:\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/ROUGE_(metric)\n",
    "- The official paper: http://www.aclweb.org/anthology/W04-1013\n",
    "- ROUGE-2, description of the updates: https://www.ideals.illinois.edu/bitstream/handle/2142/99160/rouge-2.0.pdf?sequence=2&isAllowed=y\n",
    "- Examples and explanations of the basic metrics: http://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.WyHgIXWuzCI\n",
    "- Documentation of ROUGE-2: http://rxnlp.com/rouge-2-0-usage-documentation/#.Wx9fnXWuzCI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TextRank](https://github.com/summanlp/textrank) is an extractive, graph-based, summarization algorithm which is build upon [PageRank](https://en.wikipedia.org/wiki/PageRank). This algorithm can be used both for **keyword extraction** and **sentence extraction**, but we will focus on the latter. For sentence extraction, TextRank builds a **graph** that has nodes which are sentences and it has weighted edges which indicate the similarity score between two sentences. Then, the weighted version of PageRank is used to assign scores to each node (sentence) and then rank the nodes. The nodes with highest ranking are the most \"central\" ones, or ones which many of the other sentences are similar to. This algortihm has several parameters, some of which are inherited from PageRank. The most important ones for us are **ratio - the proportion of the original text to be used in the summary** and the **similarity function** used to assign weights to the edges.\n",
    "\n",
    "Read more about TextRank:\n",
    "- The official paper: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "- Open-source implementation of TextRank: https://github.com/summanlp/textrank\n",
    "- Explanation of TextRank on Quora: https://www.quora.com/What-is-a-simple-but-detailed-explanation-of-Textrank\n",
    "- Variations of the Similarity function in TextRank: https://arxiv.org/pdf/1602.03606.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1\n",
    "> *TextRank, original implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algorithm Used                            </td><td>TextRank                                      </td></tr>\n",
       "<tr><td>ML Method                                 </td><td>unsupervised                                  </td></tr>\n",
       "<tr><td>Similarity Function                       </td><td>as in original paper, replicated below        </td></tr>\n",
       "<tr><td>Size wrt original text                    </td><td>0.2                                           </td></tr>\n",
       "<tr><td>Stopwords in pre-processing               </td><td>removed                                       </td></tr>\n",
       "<tr><td>Stopwords used in pre-processing          </td><td>original from github code, not same as in nltk</td></tr>\n",
       "<tr><td>Method of Evaluation                      </td><td>ROUGE-2-1.2.1                                 </td></tr>\n",
       "<tr><td>Metrics used                              </td><td>ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2          </td></tr>\n",
       "<tr><td>Stopwords in evaluation                   </td><td>removed                                       </td></tr>\n",
       "<tr><td>Use synonyms in evaluation                </td><td>no                                            </td></tr>\n",
       "<tr><td>Dataset for Evaluation                    </td><td>BBC News Summaries -Business                  </td></tr>\n",
       "<tr><td>Number of Documents used in the Evaluation</td><td>510                                           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec_1 = [[\"Algorithm Used \",\"TextRank\"],\n",
    "         [\"ML Method \",\"unsupervised\"],\n",
    "         [\"Similarity Function \",\" as in original paper, replicated below\"],\n",
    "         [\"Size wrt original text \", 0.2],\n",
    "         [\"Stopwords in pre-processing \", \"removed\"],\n",
    "         [\"Stopwords used in pre-processing \", \"original from github code, not same as in nltk\"], \n",
    "         [\"Method of Evaluation \", \"ROUGE-2-1.2.1\"],\n",
    "         [\"Metrics used \",\"ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2\"],\n",
    "         [\"Stopwords in evaluation \", \" removed\"],\n",
    "         [\"Use synonyms in evaluation\", \" no\"],\n",
    "         [\"Dataset for Evaluation \", \"BBC News Summaries -Business\"],\n",
    "         [\"Number of Documents used in the Evaluation \", \"510\"]]\n",
    "display(HTML(tabulate.tabulate(spec_1, tablefmt='html')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.421         </td><td>0.795            </td><td>0.543          </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.42          </td><td>0.886            </td><td>0.56           </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.427         </td><td>0.914            </td><td>0.574          </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.414         </td><td>0.881            </td><td>0.555          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_1 = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.421,0.795,0.543],\n",
    "         [\"ROUGE-SU4\",0.420,0.886,0.560],\n",
    "         [\"ROUGE-1\",0.427,0.914,0.574],\n",
    "         [\"ROUGE-2\",0.414,0.881,0.555]]\n",
    "display(HTML(tabulate.tabulate(results_1, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1\n",
    "\n",
    "**Original text**\n",
    "\n",
    "> Electrolux to export Europe jobs\n",
    ">Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries.\n",
    "The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. It did not say which facilities would be affected, but intends moving them to Asia, eastern Europe and Mexico. The company has two manufacturing sites in County Durham. It makes lawn and garden products in Newton Aycliffe, and cookers and ovens in Spennymoor. The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company.\n",
    "Electrolux's subsidiary brands include AEG, Zanussi and Frigidaire. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg. \"It looks pretty grim,\" said Swedish trades union official Ulf Carlsson. \"What are we going to end up producing in Sweden?\"\n",
    "\n",
    "**Reference Summary**\n",
    "\n",
    "> The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company. Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries. The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n",
    "**System-generated summary**\n",
    "\n",
    ">The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company.\n",
    "\"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.375         </td><td>0.75             </td><td>0.5            </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.415         </td><td>1                </td><td>0.587          </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.413         </td><td>1                </td><td>0.585          </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.414         </td><td>1                </td><td>0.586          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "example_1 = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.375,0.750,0.500],\n",
    "         [\"ROUGE-SU4\",0.415,1,0.587],\n",
    "         [\"ROUGE-1\",0.413,1,0.585],\n",
    "         [\"ROUGE-2\",0.414,1,0.586]]\n",
    "display(HTML(tabulate.tabulate(example_1, tablefmt='html')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Looking at this example, we notice that it has perfect the precision on 3 out of 4 metrics, the ones that use n-grams and skip grams. Let's remind ourselves that the precision indicates how much of the generated summary is *relevant*. Since our system summary is pretty short, only 2 sentences which are also in the reference summary, no doubt the precision got a full score. While this brings our hopes up, the recall indicates that we did not capture enough of the information. A recall of around 0.4 would mean that only 40% of the information in the reference summary was captured in our system summary. The [F-Score](https://en.wikipedia.org/wiki/F1_score) better represents our sucess, as it calculates the *harmonic mean* of the recall and precision. In our next approach we will try generating larger summaries in order to capture more information and hope to increase the recall while preserving the good precision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example1.png\" alt=\"drawing\" height=\"1000px\" width=\"3000px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](definiton1.png \"Formula 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the similarity function used in the original TextRank Implementation\n",
    "\n",
    "Here's the python code for it:\n",
    "```python\n",
    "def _get_similarity(s1, s2):\n",
    "\n",
    "\twords_sentence_one = s1.split()\n",
    "\twords_sentence_two = s2.split()\n",
    "\n",
    "\tcommon_word_count = _count_common_words(words_sentence_one, words_sentence_two)\n",
    "\n",
    "\tlog_s1 = log10(len(words_sentence_one))\n",
    "\tlog_s2 = log10(len(words_sentence_two))\n",
    "\n",
    "\tif log_s1 + log_s2 == 0:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn common_word_count / (log_s1 + log_s2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of ratio on scores\n",
    "\n",
    "Here we will see how the ratio (the proportion of the original text to be included in the summary) affects the ROUGE scores. Note that we will keep all other parameters fixed and same as before. We ran the summerizer for various ratios (represented by the x-axis on the plot below) and plotted the mean scores for all four ROUGE metrics and all three quality metrics (y-axis). \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "- We notice that the quality metrics are approximately the same across different ROUGE metrics for this dataset, which means no ROUGE score is better or worse than any other in this case.\n",
    "- Another thing that is clear from this plot is that the recall is really low for smaller ratios and then it monotonically increases, while the precision behaves in an opposite fashion. This is expected, as smaller ratio means our summary is pretty short and unlikely to capture all the important information, resulting in a small recall; on the other hand small summary guarantees high precision as it is very likely that the chosen text is relevant. F-score is more representative metric as it takes into account both recall and precision. From the plot we can see that the F-score reaches its peak at around 0.5, and then it slowly starts to decrease as a ratio higher than that would generally result in a smaller precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](accumulated_scores.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide which ratio is optimal, we plotted the precision and the recall on the same graph and chose the ratio to be approximately the x coordinate of the intersection of the precison and recall curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](prec_and_recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the ratio to be 0.42 as it a good approximation that considers the trade-off between the precison and the recall.\n",
    "\n",
    "Below are given the specifications and the result of this approach. The overall scores have significantly improved, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algorithm Used                            </td><td>TextRank                                      </td></tr>\n",
       "<tr><td>ML Method                                 </td><td>unsupervised                                  </td></tr>\n",
       "<tr><td>Similarity Function                       </td><td>as in original paper                          </td></tr>\n",
       "<tr><td>Size wrt original text                    </td><td>0.42                                          </td></tr>\n",
       "<tr><td>Stopwords in pre-processing               </td><td>removed                                       </td></tr>\n",
       "<tr><td>Stopwords used in pre-processing          </td><td>original from github code, not same as in nltk</td></tr>\n",
       "<tr><td>Method of Evaluation                      </td><td>ROUGE-2-1.2.1                                 </td></tr>\n",
       "<tr><td>Metrics used                              </td><td>ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2          </td></tr>\n",
       "<tr><td>Stopwords in evaluation                   </td><td>removed                                       </td></tr>\n",
       "<tr><td>Use synonyms in evaluation                </td><td>no                                            </td></tr>\n",
       "<tr><td>Dataset for Evaluation                    </td><td>BBC News Summaries -Business                  </td></tr>\n",
       "<tr><td>Number of Documents used in the Evaluation</td><td>510                                           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec_ratio042 = [[\"Algorithm Used \",\"TextRank\"],\n",
    "         [\"ML Method \",\"unsupervised\"],\n",
    "         [\"Similarity Function \",\" as in original paper\"],\n",
    "         [\"Size wrt original text \", 0.42],\n",
    "         [\"Stopwords in pre-processing \", \"removed\"],\n",
    "         [\"Stopwords used in pre-processing \", \"original from github code, not same as in nltk\"], \n",
    "         [\"Method of Evaluation \", \"ROUGE-2-1.2.1\"],\n",
    "         [\"Metrics used \",\"ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2\"],\n",
    "         [\"Stopwords in evaluation \", \" removed\"],\n",
    "         [\"Use synonyms in evaluation\", \" no\"],\n",
    "         [\"Dataset for Evaluation \", \"BBC News Summaries -Business\"],\n",
    "         [\"Number of Documents used in the Evaluation \", \"510\"]]\n",
    "display(HTML(tabulate.tabulate(spec_ratio042, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.686         </td><td>0.697            </td><td>0.687          </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.766         </td><td>0.776            </td><td>0.766          </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.785         </td><td>0.798            </td><td>0.788          </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.759         </td><td>0.77             </td><td>0.76           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_ratio042 = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.686,0.697,0.687],\n",
    "         [\"ROUGE-SU4\",0.766,0.776,0.766],\n",
    "         [\"ROUGE-1\",0.785,0.798,0.788],\n",
    "         [\"ROUGE-2\",0.759,0.770, 0.760]]\n",
    "display(HTML(tabulate.tabulate(results_ratio042, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the system generated summary looks like compared to the reference summary, with our new ratio 0.42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2\n",
    "\n",
    "**Reference Summary**\n",
    "\n",
    "> The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company. Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries. The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n",
    "**System-generated summary**\n",
    "\n",
    "> Electrolux to export Europe jobs\n",
    "\n",
    "> Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries.\n",
    "The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America.\n",
    "The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company.\n",
    "\"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We significantly increased our evaluation results by setting the parameter **ratio** to the value that maximized the scores given our current algorithm. However, we must realize that the success of this particular ratio is data-dependent, in particular the relative length of the reference summaries. The ratio 0.42 might be too high depending on our needs. If all we need is a one to two sentences summary of a relatively large text, then clearly we would set the ratio to something lower. If we still want to achieve high scores, we must provide references summaries which will be representative of the way we want our system generated summaries to look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2\n",
    "> *TextRank, Cosine Similarity, Doc2Vec,Gensim*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach we will use the same algorithm with a different similarity function. We will try [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function applied on [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) vectors from [gensim](https://radimrehurek.com/gensim/) library. Doc2Vec is similar to the popular [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) representation, except that now we can represent larger units of text such as phrases, sentences and full documents as numerical vectors. Due to the new vector representation of sentences we had to make several adjustments to the original TextRank algorithm.\n",
    "\n",
    "Read more about Doc2Vec:\n",
    "- Paper: https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "- Tutorial (deprecated but still useful): https://rare-technologies.com/doc2vec-tutorial/\n",
    "- Another tutorial: https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Summary**\n",
    "\n",
    "> The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company. Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries. The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n",
    "**System-generated Summary**\n",
    "\n",
    "> The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America.\n",
    "It did not say which facilities would be affected, but intends moving them to Asia, eastern Europe and Mexico.\n",
    "The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company.\n",
    "The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009.\n",
    "\"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\"It looks pretty grim,\" said Swedish trades union official Ulf Carlsson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algorithm Used                            </td><td>TextRank                            </td></tr>\n",
       "<tr><td>ML Method                                 </td><td>unsupervised                        </td></tr>\n",
       "<tr><td>Similarity Function                       </td><td>cosine similarity                   </td></tr>\n",
       "<tr><td>Sentence representation                   </td><td>Doc2Vec from gensim                 </td></tr>\n",
       "<tr><td>Size wrt original text                    </td><td>/                                   </td></tr>\n",
       "<tr><td>Stopwords in pre-processing               </td><td>not removed                         </td></tr>\n",
       "<tr><td>Stopwords used in pre-processing          </td><td>N/A                                 </td></tr>\n",
       "<tr><td>Method of Evaluation                      </td><td>ROUGE-2-1.2.1                       </td></tr>\n",
       "<tr><td>Metrics used                              </td><td>ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2</td></tr>\n",
       "<tr><td>Stopwords in evaluation                   </td><td>removed                             </td></tr>\n",
       "<tr><td>Use synonyms in evaluation                </td><td>no                                  </td></tr>\n",
       "<tr><td>Dataset for Evaluation                    </td><td>BBC News Summaries -Business        </td></tr>\n",
       "<tr><td>Number of Documents used in the Evaluation</td><td>510                                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec_doc2vec = [[\"Algorithm Used \",\"TextRank\"],\n",
    "         [\"ML Method \",\"unsupervised\"],\n",
    "         [\"Similarity Function \",\" cosine similarity\"],\n",
    "         [\"Sentence representation\", \"Doc2Vec from gensim\"],\n",
    "         [\"Size wrt original text \", \"/\"],\n",
    "         [\"Stopwords in pre-processing \", \"not removed\"],\n",
    "         [\"Stopwords used in pre-processing \", \"N/A\"], \n",
    "         [\"Method of Evaluation \", \"ROUGE-2-1.2.1\"],\n",
    "         [\"Metrics used \",\"ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2\"],\n",
    "         [\"Stopwords in evaluation \", \" removed\"],\n",
    "         [\"Use synonyms in evaluation\", \" no\"],\n",
    "         [\"Dataset for Evaluation \", \"BBC News Summaries -Business\"],\n",
    "         [\"Number of Documents used in the Evaluation \", \"510\"]]\n",
    "display(HTML(tabulate.tabulate(spec_doc2vec, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried this approach for various values of the ratio as well. Note that these results are for the case when we **did not remove the stopwords in pre-processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](doc2vec_full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for ratio=0.5 are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](doc2vec_prec_and_recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.593         </td><td>0.478            </td><td>0.525          </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.602         </td><td>0.52             </td><td>0.552          </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.666         </td><td>0.569            </td><td>0.609          </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.588         </td><td>0.504            </td><td>0.538          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_doc2vec_ratio = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.593,0.478,0.525],\n",
    "         [\"ROUGE-SU4\",0.602,0.520,0.552],\n",
    "         [\"ROUGE-1\",0.666,0.569,0.609],\n",
    "         [\"ROUGE-2\",0.588,0.504,0.538 ]]\n",
    "display(HTML(tabulate.tabulate(results_doc2vec_ratio, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results in the case where we  **do remove the stopwords in pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algorithm Used                            </td><td>TextRank                            </td></tr>\n",
       "<tr><td>ML Method                                 </td><td>unsupervised                        </td></tr>\n",
       "<tr><td>Similarity Function                       </td><td>cosine similarity                   </td></tr>\n",
       "<tr><td>Sentence representation                   </td><td>Doc2Vec from gensim                 </td></tr>\n",
       "<tr><td>Size wrt original text                    </td><td>/                                   </td></tr>\n",
       "<tr><td>Stopwords in pre-processing               </td><td>removed                             </td></tr>\n",
       "<tr><td>Stopwords used in pre-processing          </td><td>nltk stopwords                      </td></tr>\n",
       "<tr><td>Method of Evaluation                      </td><td>ROUGE-2-1.2.1                       </td></tr>\n",
       "<tr><td>Metrics used                              </td><td>ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2</td></tr>\n",
       "<tr><td>Stopwords in evaluation                   </td><td>removed                             </td></tr>\n",
       "<tr><td>Use synonyms in evaluation                </td><td>no                                  </td></tr>\n",
       "<tr><td>Dataset for Evaluation                    </td><td>BBC News Summaries -Business        </td></tr>\n",
       "<tr><td>Number of Documents used in the Evaluation</td><td>510                                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec_doc2vec = [[\"Algorithm Used \",\"TextRank\"],\n",
    "         [\"ML Method \",\"unsupervised\"],\n",
    "         [\"Similarity Function \",\" cosine similarity\"],\n",
    "         [\"Sentence representation\", \"Doc2Vec from gensim\"],\n",
    "         [\"Size wrt original text \", \"/\"],\n",
    "         [\"Stopwords in pre-processing \", \"removed\"],\n",
    "         [\"Stopwords used in pre-processing \", \"nltk stopwords\"], \n",
    "         [\"Method of Evaluation \", \"ROUGE-2-1.2.1\"],\n",
    "         [\"Metrics used \",\"ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2\"],\n",
    "         [\"Stopwords in evaluation \", \" removed\"],\n",
    "         [\"Use synonyms in evaluation\", \" no\"],\n",
    "         [\"Dataset for Evaluation \", \"BBC News Summaries -Business\"],\n",
    "         [\"Number of Documents used in the Evaluation \", \"510\"]]\n",
    "display(HTML(tabulate.tabulate(spec_doc2vec, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](removed_stopwords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](removed_stopwords_pr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the scores we get for ratio=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.656         </td><td>0.427            </td><td>0.514          </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.681         </td><td>0.467            </td><td>0.55           </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.737         </td><td>0.49             </td><td>0.586          </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.659         </td><td>0.462            </td><td>0.54           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_doc2vec_ratio05 = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.656,0.427,0.514],\n",
    "         [\"ROUGE-SU4\",0.681,0.467,0.55],\n",
    "         [\"ROUGE-1\",0.737,0.490,0.586],\n",
    "         [\"ROUGE-2\",0.659,0.462, 0.540]]\n",
    "display(HTML(tabulate.tabulate(results_doc2vec_ratio05, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "In our second approach we used cosine similarity between sentences represented using Doc2Vec. We can see that numerically the results are slightly worse than using the original similarity function. However, the outcome is pretty positive in this approach as well. Looking at the summaries, we can conclude that they generally \"make sense\" and extract relevant and informative sentences that do capture the main ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3\n",
    "> *LexRank, Continious LexRank (weighted edges), tf-idf, stochaistic matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LexRank is very similar to TextRank. The main difference is that LexRank uses IDF-modified cosine similarity to calculate the similarity between two sentences. A Markov Matrix is created containg the intra-sentence cosine similarity and this represents the transition matrix in the PageRank Algorithm. When the sentence graph has weighted edges, the algorithm is called Continious LexRank. Another version of LexRank is centroid-based, where each edge is treated as a vote to determine the overall centrallity of each node and is therefore unweighted. However, in general we know that not all relationships are equally important. That's why we prefer the continious model (weighted edges).\n",
    "\n",
    "Read more about LexRank:\n",
    "- Official paper: https://www.aaai.org/Papers/JAIR/Vol22/JAIR-2214.pdf\n",
    "- Open-source implementation: https://github.com/wikibusiness/lexrank\n",
    "- TextRank vs LexRank on wikipedia: https://en.wikipedia.org/wiki/Automatic_summarization#TextRank_and_LexRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a LexRank summary looks like for ratio=0.4.\n",
    "\n",
    "**Note**: The original algorithm returns the selected sentences in the order in which they were ranked. We put them in the order in which they appear in the text, as that makes the summary more natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3\n",
    "\n",
    "**Reference Summary**\n",
    "\n",
    "> The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company. Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries. The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n",
    "**System-generated Summary**\n",
    "\n",
    "> Electrolux to export Europe jobs\n",
    "\n",
    "> Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries.\n",
    "The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America.\n",
    "The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company.\n",
    "\"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algorithm Used                            </td><td>LexRank                                         </td></tr>\n",
       "<tr><td>ML Method                                 </td><td>unsupervised                                    </td></tr>\n",
       "<tr><td>Similarity Function                       </td><td>cosine similarity of tf-idf vectors, given below</td></tr>\n",
       "<tr><td>Size wrt original text                    </td><td>/                                               </td></tr>\n",
       "<tr><td>Stopwords in pre-processing               </td><td>removed                                         </td></tr>\n",
       "<tr><td>Stopwords used in pre-processing          </td><td>original from github code, same as in nltk(?)   </td></tr>\n",
       "<tr><td>Method of Evaluation                      </td><td>ROUGE-2-1.2.1                                   </td></tr>\n",
       "<tr><td>Metrics used                              </td><td>ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2            </td></tr>\n",
       "<tr><td>Stopwords in evaluation                   </td><td>removed                                         </td></tr>\n",
       "<tr><td>Use synonyms in evaluation                </td><td>no                                              </td></tr>\n",
       "<tr><td>Dataset for Evaluation                    </td><td>BBC News Summaries -Business                    </td></tr>\n",
       "<tr><td>Number of Documents used in the Evaluation</td><td>510                                             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec_lex = [[\"Algorithm Used \",\"LexRank\"],\n",
    "         [\"ML Method \",\"unsupervised\"],\n",
    "         [\"Similarity Function \",\" cosine similarity of tf-idf vectors, given below\"],\n",
    "         [\"Size wrt original text \", \"/\"],\n",
    "         [\"Stopwords in pre-processing \", \"removed\"],\n",
    "         [\"Stopwords used in pre-processing \", \"original from github code, same as in nltk(?)\"], \n",
    "         [\"Method of Evaluation \", \"ROUGE-2-1.2.1\"],\n",
    "         [\"Metrics used \",\"ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2\"],\n",
    "         [\"Stopwords in evaluation \", \" removed\"],\n",
    "         [\"Use synonyms in evaluation\", \" no\"],\n",
    "         [\"Dataset for Evaluation \", \"BBC News Summaries -Business\"],\n",
    "         [\"Number of Documents used in the Evaluation \", \"510\"]]\n",
    "display(HTML(tabulate.tabulate(spec_lex, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](idf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the evaluation plots for our third approach using LexRank:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lexrank_full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lex_pr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.694         </td><td>0.67             </td><td>0.678          </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.772         </td><td>0.745            </td><td>0.754          </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.795         </td><td>0.745            </td><td>0.77           </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.769         </td><td>0.737            </td><td>0.748          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_lexrank_ratio042 = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.694,0.670,0.678],\n",
    "         [\"ROUGE-SU4\",0.772,0.745,0.754],\n",
    "         [\"ROUGE-1\",0.795,0.745,0.770],\n",
    "         [\"ROUGE-2\",0.769,0.737, 0.748]]\n",
    "display(HTML(tabulate.tabulate(results_lexrank_ratio042, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "We can see that the results of LexRank are comparable to those of TextRank, the latter performing negligibly better. For LexRank we needed to calculate the tf-idf similarity matrix which required us to have corpora of documents, describing similar topics. In our dataset, BBC news-business this was easy as all the documents were related to business. When working with real data we might prefer TextRank as the algorithm does not consider other documents when summarizing a certain document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 4\n",
    "> *TextRank, LDA (Latent Dirichlet Allocation), cosine similarity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Dirichlet allocation (LDA)** is a topic model that generates topics based on word frequency from a set of documents. The model, among other things, takes as input a list of documents (strings) and number of topics and can give us probability distribution over topics for each document. In our case, we have set a document to be a single sentence, so that we can have vector representation of the sentence. As mentioned, the feautures of these vectors are probabilities over topics. \n",
    "\n",
    "Read more about LDA:\n",
    "\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "- Scikit-learn documentation: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "- Quora explanation of LDA: https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation\n",
    "- Tutorial on topic modeling: https://nlpforhackers.io/topic-modeling/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algorithm Used                            </td><td>TextRank                                                               </td></tr>\n",
       "<tr><td>ML Method                                 </td><td>unsupervised                                                           </td></tr>\n",
       "<tr><td>Similarity Function                       </td><td>cosine similarity of LDA topic vectors + modified original (read below)</td></tr>\n",
       "<tr><td>Size wrt original text                    </td><td>/                                                                      </td></tr>\n",
       "<tr><td>number of topics for LDA                  </td><td>3                                                                      </td></tr>\n",
       "<tr><td>Stopwords in pre-processing               </td><td>removed                                                                </td></tr>\n",
       "<tr><td>Stopwords used in pre-processing          </td><td>original from github code, same as in nltk(?)                          </td></tr>\n",
       "<tr><td>Method of Evaluation                      </td><td>ROUGE-2-1.2.1                                                          </td></tr>\n",
       "<tr><td>Metrics used                              </td><td>ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2                                   </td></tr>\n",
       "<tr><td>Stopwords in evaluation                   </td><td>removed                                                                </td></tr>\n",
       "<tr><td>Use synonyms in evaluation                </td><td>no                                                                     </td></tr>\n",
       "<tr><td>Dataset for Evaluation                    </td><td>BBC News Summaries -Business                                           </td></tr>\n",
       "<tr><td>Number of Documents used in the Evaluation</td><td>510                                                                    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec_lda= [[\"Algorithm Used \",\"TextRank\"],\n",
    "         [\"ML Method \",\"unsupervised\"],\n",
    "         [\"Similarity Function \",\" cosine similarity of LDA topic vectors + modified original (read below)\"],\n",
    "         [\"Size wrt original text \", \"/\"],\n",
    "         [\"number of topics for LDA \", \"3\"],  \n",
    "         [\"Stopwords in pre-processing \", \"removed\"],\n",
    "         [\"Stopwords used in pre-processing \", \"original from github code, same as in nltk(?)\"], \n",
    "         [\"Method of Evaluation \", \"ROUGE-2-1.2.1\"],\n",
    "         [\"Metrics used \",\"ROUGE-L, ROUGE-SU4, ROUGE-1, ROUGE-2\"],\n",
    "         [\"Stopwords in evaluation \", \" removed\"],\n",
    "         [\"Use synonyms in evaluation\", \" no\"],\n",
    "         [\"Dataset for Evaluation \", \"BBC News Summaries -Business\"],\n",
    "         [\"Number of Documents used in the Evaluation \", \"510\"]]\n",
    "display(HTML(tabulate.tabulate(spec_lda, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "\n",
    "### Initial approach\n",
    "\n",
    "\n",
    "The way we used LDA is as follows: We took the document we want to summarize and separated it into sentences. Those sentences are the input to the LDA model. It is worth mentioning that for good results one needs a larger amount of input data; our testing documents have on average 15 sentences. We also set the number of topics to be 3. Normally this should be a larger number, but since the number of sentences is small, 3 seems to be a good number. We trained the model and for each sentence we have obtained a vector of 3 values representing probability distribution on each topic for that particular sentence. The main similarity function we used was the cosine similarity applied on these vectors. Then, the PageRank algorithm was used as usual to rank the sentences. **This similarity function by itself did not give satisfying results, even when trying several values for the number of topics**. The reason for this might be because we were calculating topical similarity, but not each topic is important. For example, in the article we've used throughout this notebook (the one about Electrolux) the system summary focused too much on description of the company, considering as important sentences like: *\"The company has two manufacturing sites in County Durham.\"* and   *\"It makes lawn and garden products in Newton Aycliffe, and cookers and ovens in Spennymoor.\"* Considering the main information that the article is trying to convey, it is clear that these sentences are not crucial enough to be included in a relatively short summary. This is why we decided to change a little detail when calculating the score of the sentences, after applying PageRank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4\n",
    "\n",
    "**Reference Summary**\n",
    "\n",
    "> The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company. Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries. The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n",
    "**System-generated Summary initial approach**\n",
    "\n",
    "> Electrolux to export Europe jobs\n",
    "It did not say which facilities would be affected, but intends moving them to Asia, eastern Europe and Mexico.\n",
    "The company has two manufacturing sites in County Durham.\n",
    "Electrolux's subsidiary brands include AEG, Zanussi and Frigidaire.\n",
    "\"It looks pretty grim,\" said Swedish trades union official Ulf Carlsson.\n",
    "\"What are we going to end up producing in Sweden?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved approach\n",
    "\n",
    "In our improved approach we used the same LDA vectors and cosine similarity, and applied PageRank and got the initial scores for sentences. The change we made was adding weighted similarity scores between the selected sentence and its similarity to the first and second sentence in the document. Here we used the original similarity function provided in the paper about TextRank (and available at he beginning of this notebook). Basically if the score of sentence $i$ was $score[i]$ we did:\n",
    "$$score[i]+=0.6*originalSimilarity(sentence[i],sentence[0])+0.4*originalSimilarity(sentence[i],sentence[1])$$\n",
    "With this, we emphasized the importance of the first and the second sentence, implying that sentences more similar to the initial sentences are more important. This is a common and generally safe assumption in text summarization which ususally gives good results. This approach can be further easily modified by tuning the numerous parameteres we've used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5\n",
    "\n",
    "**Reference Summary**\n",
    "\n",
    "> The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company. Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries. The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America. The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009. \"We see that about half the plants in high-cost countries - that is around 10 - are at risk,\" said Electrolux chief executive Hans Straberg.\n",
    "\n",
    "**System-generated Summary improved approach**\n",
    "\n",
    "> Electrolux to export Europe jobs\n",
    "\n",
    "> Electrolux saw its shares rise 14% on Tuesday after it said it would be shifting more of its manufacturing to low-cost countries.\n",
    "The Swedish firm, the world's largest maker of home appliances, said it is to relocate about 10 of its 27 plants in western Europe and North America.\n",
    "It did not say which facilities would be affected, but intends moving them to Asia, eastern Europe and Mexico.\n",
    "The Newton Aycliffe plant could also be affected by Electrolux's separate announcement that it is to spin-off its outdoor products unit into a new separate company.\n",
    "The company said it was speeding up its restructuring programme, which aims to save between £190m and £265m annually from 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots summarize the results for the *improved approach*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lda_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lda_pr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results for ratio = 0.45 with the improved approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>         </td><td>Average Recall</td><td>Average Precision</td><td>Average F-Score</td></tr>\n",
       "<tr><td>ROUGE-L  </td><td>0.577         </td><td>0.588            </td><td>0.578          </td></tr>\n",
       "<tr><td>ROUGE-SU4</td><td>0.609         </td><td>0.642            </td><td>0.62           </td></tr>\n",
       "<tr><td>ROUGE-1  </td><td>0.649         </td><td>0.673            </td><td>0.657          </td></tr>\n",
       "<tr><td>ROUGE-2  </td><td>0.599         </td><td>0.625            </td><td>0.608          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_lda_045 = [[\"   \",\"Average Recall\",\"Average Precision\", \"Average F-Score\"],\n",
    "         [\"ROUGE-L\",0.577,0.588,0.578],\n",
    "         [\"ROUGE-SU4\",0.609,0.642,0.620],\n",
    "         [\"ROUGE-1\",0.649,0.673,0.657],\n",
    "         [\"ROUGE-2\",0.599,0.625,0.608]]\n",
    "display(HTML(tabulate.tabulate(results_lda_045, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing approaches on \"real-world\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the text which we want to summarize:**\n",
    "> Mexico will have a tough year. \n",
    "Indeed, 2018 will be a defining moment for the country's longer-term outlook, which will depend on the outcome of nafta renegotiation and the country's 1 july presidential election. \n",
    "Both carry significant market risks.\n",
    "First, nafta.\n",
    "A successful renegotiation in 2018 is still possible.\n",
    "It's far from certain Trump would act on his threat to initiate withdrawal from the deal; even if he does, this would be a ploy to enhance us leverage in future negotiations rather than an attempt to destroy the agreement.\n",
    "Unfortunately, that's where the good news stops.\n",
    "Renegotiation of the 23-year-old deal started last august and dominated the second half of the year, with scant results. \n",
    "Increasingly protectionist US proposals have slowed negotiations. \n",
    "Canada, the US, and Mexico share the goal of reaching a deal to revamp the agreement by the end of march, before the presidential campaign begins in Mexico. \n",
    "But successful renegotiation depends on the US softening its stance; Mexico and Canada have few incentives to compromise with the Trump administration, as they know the US business community firmly opposes nafta withdrawal.\n",
    "If there is no deal or if Trump initiates a withdrawal process, this would not mark the end of nafta, but it would put an end to negotiations. \n",
    "Canada and Mexico would, at least initially, walk away, creating uncertainty over billions of dollars of economic activity in the world's most prosperous region. \n",
    "Though the pain would be shared, the Mexican economy and those who invest in it would suffer disproportionately, given the country's deep reliance on trade with the US.\n",
    "The nafta debate and the country's presidential election are likely to overlap and amplify the risks each presents. \n",
    "Once the presidential campaign starts in march, it will become very hard for government negotiators to agree to meaningful compromises without seeming to bow to the US hegemonic neighbor.\n",
    "In addition, the campaign's frontrunner is Andres Manuel Lopez Obrador, known as Amlo, offers anti-US rhetoric and a statist economic policy platform.\n",
    "Voter anger at government is running high, thanks to high-profile corruption cases, a deterioration of the security situation, and sluggish economic growth. \n",
    "Public demand for change favors Amlo, and though the ruling institutional revolutionary party (pri) candidate, finance minister Jose Antonio Meade, appeals to independent voters, his association with unpopular president Enrique Pena Nieto will be a burden on his candidacy.\n",
    "Lopez Obrador is not as radical as some rivals portray him, but he represents a fundamental break with the investor-friendly economic model implemented in Mexico since the 1980s, particularly for the recently enacted opening of the energy sector to private foreign investors. \n",
    "Fiscal constraints and a lack of congressional majorities would limit what he can achieve, but an Amlo presidency, particularly if nafta's future remains uncertain, would bring significant market risk to Mexico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will not try to evaluate numerically, as that is a bit hard in this case. This is the human-produced summary of the above text:**\n",
    "> Unsurprisingly, Mexico runs a high risk of getting between a rock and a hard place if Trump decides to withdraw from Nafta, which is not a remote possibility. In all of this, Mexican elections are taking place on July 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the ratio to be be 0.2 in **all cases** below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1\n",
    "> *TextRank, original implementation*\n",
    "\n",
    "> Canada, the US, and Mexico share the goal of reaching a deal to revamp the agreement by the end of march, before the presidential campaign begins in Mexico.\n",
    "But successful renegotiation depends on the US softening its stance; Mexico and Canada have few incentives to compromise with the Trump administration, as they know the US business community firmly opposes nafta withdrawal.\n",
    "If there is no deal or if Trump initiates a withdrawal process, this would not mark the end of nafta, but it would put an end to negotiations.\n",
    "Fiscal constraints and a lack of congressional majorities would limit what he can achieve, but an Amlo presidency, particularly if nafta's future remains uncertain, would bring significant market risk to Mexico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2\n",
    "> *TextRank, Cosine Similarity, Doc2Vec,Gensim*\n",
    "\n",
    "> Stopwords not removed in pre-processing\n",
    "\n",
    "> It's far from certain Trump would act on his threat to initiate withdrawal from the deal; even if he does, this would be a ploy to enhance us leverage in future negotiations rather than an attempt to destroy the agreement.\n",
    "Canada, the US, and Mexico share the goal of reaching a deal to revamp the agreement by the end of march, before the presidential campaign begins in Mexico.\n",
    "Lopez Obrador is not as radical as some rivals portray him, but he represents a fundamental break with the investor-friendly economic model implemented in Mexico since the 1980s, particularly for the recently enacted opening of the energy sector to private foreign investors.\n",
    "Fiscal constraints and a lack of congressional majorities would limit what he can achieve, but an Amlo presidency, particularly if nafta's future remains uncertain, would bring significant market risk to Mexico.\n",
    "\n",
    "\n",
    "> Stopwords removed in pre-processing\n",
    "\n",
    "> It's far from certain Trump would act on his threat to initiate withdrawal from the deal; even if he does, this would be a ploy to enhance us leverage in future negotiations rather than an attempt to destroy the agreement.\n",
    "Public demand for change favors Amlo, and though the ruling institutional revolutionary party (pri) candidate, finance minister Jose Antonio Meade, appeals to independent voters, his association with unpopular president Enrique Pena Nieto will be a burden on his candidacy.\n",
    "Lopez Obrador is not as radical as some rivals portray him, but he represents a fundamental break with the investor-friendly economic model implemented in Mexico since the 1980s, particularly for the recently enacted opening of the energy sector to private foreign investors. \n",
    "Fiscal constraints and a lack of congressional majorities would limit what he can achieve, but an Amlo presidency, particularly if nafta's future remains uncertain, would bring significant market risk to Mexico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3\n",
    "> *LexRank, Continious LexRank (weighted edges), tf-idf, stochaistic matrix*\n",
    "\n",
    "> Indeed, 2018 will be a defining moment for the country's longer-term outlook, which will depend on the outcome of nafta renegotiation and the country's 1 july presidential election. \n",
    "First, nafta.\n",
    "Canada, the US, and Mexico share the goal of reaching a deal to revamp the agreement by the end of march, before the presidential campaign begins in Mexico. \n",
    "But successful renegotiation depends on the US softening its stance; Mexico and Canada have few incentives to compromise with the Trump administration, as they know the US business community firmly opposes nafta withdrawal.\n",
    "If there is no deal or if Trump initiates a withdrawal process, this would not mark the end of nafta, but it would put an end to negotiations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 4\n",
    "> *TextRank, LDA (Latent Dirichlet Allocation), cosine similarity*\n",
    "\n",
    "> Mexico will have a tough year.\n",
    "Indeed, 2018 will be a defining moment for the country's longer-term outlook, which will depend on the outcome of nafta renegotiation and the country's 1 july presidential election.\n",
    "Canada, the US, and Mexico share the goal of reaching a deal to revamp the agreement by the end of march, before the presidential campaign begins in Mexico.\n",
    "Public demand for change favors Amlo, and though the ruling institutional revolutionary party (pri) candidate, finance minister Jose Antonio Meade, appeals to independent voters, his association with unpopular president Enrique Pena Nieto will be a burden on his candidacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
